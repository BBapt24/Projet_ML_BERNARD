# Projet_ML_BERNARD

## *Introduction*

Depuis leur introduction en 2017, les réseaux de neurones de type transformers semblent avoir pris le pas sur tous les autres types de réseaux et ce, quel que soit le type de problème à résoudre. C'est pourquoi, dans le cadre de mon projet de Machine Learning, je propose de s'atteler à un problème classique de classification d'image en étudiant deux architectures de réseaux face à la base de données CIFAR-10 (*CIFAR-10 se compose de 60000 images couleur 32x32 réparties en 10 classes, avec 6000 images par classe. Il y a 50000 images d'apprentissage et 10000 images de test.*). 


La première architecture sera une version simple de ResNet, un type de réseaux qui a été conçu pour résoudre ce type de problème.
La seconde architecture sera un modèle de Transformers dont je détaillerai l'architecture et la création par la suite.

## *I- ResNet*

ResNet (Residual Networks) est une architecture de réseau neuronal profond qui a introduit le concept de bloc résiduel, permettant de résoudre le problème de la disparition du gradient dans les réseaux profonds. L'idée clé derrière ResNet est d'introduire des connexions résiduelles (skip connections) entre les couches, ce qui permet au modèle d'apprendre les résidus plutôt que les fonctions directes. 

Introduit en 2015, cette architecture est donc très pratique pour réaliser 


![image](https://github.com/BBapt24/Projet_ML_BERNARD/assets/150921474/675134cb-5e3d-46f4-a30e-cd60ce28cb3f)
**Figure 1 - Descriptif des différentes architectures de ResNet [1]**




## *II- Transformers*




## *Conclusion*



## *Bibliographie*

[1] https://arxiv.org/pdf/1512.03385.pdf (papier de recherche introduisant le modèle ResNet).

[2] https://arxiv.org/pdf/2010.11929v2.pdf (papier de recherche introduisant les Vision Transformers).


